{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. **Prunning** - is a model compression technique that involves removing the unnecessary connections or weights from a neural network. The goal of pruning is to reduce the size of the network while maintaining its accuracy. Pruning can be done in different ways, such as removing the smallest weights, or removing the weights that have the least impact on the output of the network."
      ],
      "metadata": {
        "id": "-IFrbTGgqQGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Quantization** - is a model compression technique that reduces the precision of the weights and activations of a neural network. In other words, it involves representing the weights and activations of a neural network using fewer bits than their original precision."
      ],
      "metadata": {
        "id": "_eJaAzcZq3Wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Knowledge Distillation** - is a model compression technique that involves transferring the knowledge from a large, complex neural network (teacher network) to a smaller, simpler neural network (student network). The goal of knowledge distillation is to reduce the size of the network while maintaining its accuracy by leveraging the knowledge learned by the teacher network."
      ],
      "metadata": {
        "id": "XR0J7u5iuXsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Low Rank Factorization** - identifies redundant parameters of deep neural networks by employing the matrix and tensor decomposition. When reducing the model size is necessary, a low-rank factorization technique helps by decomposing a large matrix into smaller matrices. In this way the number of parameters and calculations significantly lowering."
      ],
      "metadata": {
        "id": "kZlgpeTQy7zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools:\n",
        "1. INC - prunning, quantization, distilation\n",
        "https://intel.github.io/neural-compressor/latest/docs/source/pruning.html#pruning\n",
        "https://intel.github.io/neural-compressor/latest/docs/source/Welcome.html#installation\n",
        "https://intel.github.io/neural-compressor/latest/docs/source/distillation.html#get-started-with-distillation-api\n",
        "2. PyTorch Pruner\n",
        "https://pytorch.org/docs/stable/nn.html#quantized-functions\n",
        "3. TensorFlow Model optimization - prunning, quantization\n",
        "https://www.tensorflow.org/model_optimization\n",
        "4. NNI - prunning, quantization\n",
        "https://nni.readthedocs.io/en/v1.9/Compression/QuickStart.html\n",
        "5. AIMET - quantization, low level factorization\n",
        "https://quic.github.io/aimet-pages/AimetDocs/user_guide/model_compression.html"
      ],
      "metadata": {
        "id": "cRSLThGtvmwY"
      }
    }
  ]
}